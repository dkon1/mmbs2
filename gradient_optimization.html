
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Optimization using gradients &#8212; Mathematical Methods for Biological Sciences (part 2)</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Principal Component Analysis" href="PCA.html" />
    <link rel="prev" title="Linear regression" href="linear_regression.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Mathematical Methods for Biological Sciences (part 2)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Mathematical Methods for Biology Part 2: Algorithms
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="linear_regression.html">
   Linear regression
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Optimization using gradients
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PCA.html">
   Principal Component Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="fourier_series.html">
   Fourier series: decomposition by frequency
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/gradient_optimization.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fgradient_optimization.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization-with-one-variable">
   Optimization with one variable
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#golden-section-search">
     golden section search
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bisection-method">
     bisection method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#secant-method">
     secant method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#newton-raphson-method">
     Newton-Raphson method
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization-of-multivariable-functions-using-derivatives">
   Optimization of multivariable functions using derivatives
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multivariable-optimization-problem">
     multivariable optimization problem
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-and-contours">
     gradient and contours
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     gradient descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Newton-Raphson method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#levenberg-marquardt-method">
     Levenberg-Marquardt method
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Optimization using gradients</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization-with-one-variable">
   Optimization with one variable
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#golden-section-search">
     golden section search
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bisection-method">
     bisection method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#secant-method">
     secant method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#newton-raphson-method">
     Newton-Raphson method
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization-of-multivariable-functions-using-derivatives">
   Optimization of multivariable functions using derivatives
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multivariable-optimization-problem">
     multivariable optimization problem
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-and-contours">
     gradient and contours
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     gradient descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Newton-Raphson method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#levenberg-marquardt-method">
     Levenberg-Marquardt method
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="optimization-using-gradients">
<h1>Optimization using gradients<a class="headerlink" href="#optimization-using-gradients" title="Permalink to this headline">¶</a></h1>
<p>One often has to find the maximum or minimum of a function that describes an important biological property. Examples include:</p>
<ul class="simple">
<li><p>Free energy of a protein as function of its conformation (the folding problem)</p></li>
<li><p>Evolutionary fitness as a function of the genome</p></li>
<li><p>Optimizing the dose of therapeutic radiation or chemotherapy, to affect the maximal fraction of tumor cells and the minimal number of healthy cells</p></li>
</ul>
<p>In all these cases, there is the same fundamental problem: given a complex function of many variables, find the values of the variables which optimize the function, that is, for which it attains its maximum or minimum, depending on the question.</p>
<p>Some terminology: <span class="math notranslate nohighlight">\(f\)</span> is known as the <em>objective function</em>, and the quantities the function depends on (<span class="math notranslate nohighlight">\(x_1, x_2, ..., x_n\)</span>) are the governing variables.</p>
<p>It is important to note the notion of a minimum or a maximum is local: it is a point which is lowest or highest in some neighborhood. But very often we are interested in a global optimum - the best possible solution to our problem. The first problem is not too difficult, while the second is practically impossible for a complex function, because, aside from the areas explored by minimization, there is no way to guarantee that there is no better optimum elsewhere. This is the tragic state of optimizers everywhere - they have to live with the uncertainty.</p>
<div class="section" id="optimization-with-one-variable">
<h2>Optimization with one variable<a class="headerlink" href="#optimization-with-one-variable" title="Permalink to this headline">¶</a></h2>
<div class="section" id="golden-section-search">
<h3>golden section search<a class="headerlink" href="#golden-section-search" title="Permalink to this headline">¶</a></h3>
<p>Let us first consider searching for the minimum (or maximum, the problems are really the same except for a minus sign) of a function of one variable <span class="math notranslate nohighlight">\(f(x)\)</span>. The goal is to locate the minimum value of the function within a certain interval on the <span class="math notranslate nohighlight">\(x\)</span>-axis, to a desired tolerance. We assume only that we can evaluate the function at any given value of <span class="math notranslate nohighlight">\(x\)</span>. Here is the outline of the algorithm:</p>
<div class="tip admonition">
<p class="admonition-title">Golden section search</p>
<ol class="simple">
<li><p>start with three points <span class="math notranslate nohighlight">\(x_1,x_2,x_3\)</span> <span class="math notranslate nohighlight">\((x_1&lt;x_2&lt;x_3)\)</span>, such that <span class="math notranslate nohighlight">\(f(x_2) &lt; f(x_1)\)</span> and <span class="math notranslate nohighlight">\(f(x_2) &lt; f(x_3)\)</span></p></li>
<li><p>choose a point <span class="math notranslate nohighlight">\(x_4\)</span> in the larger of the intervals <span class="math notranslate nohighlight">\((x_1,x_2)\)</span> or <span class="math notranslate nohighlight">\((x_2,x_3)\)</span> (let’s assume it’s <span class="math notranslate nohighlight">\((x_1,x_2)\)</span>, the process is the same for the other case)</p></li>
<li><p>if <span class="math notranslate nohighlight">\(f(x_4) &gt; f(x_2)\)</span>, replace <span class="math notranslate nohighlight">\(x_1\)</span> with <span class="math notranslate nohighlight">\(x_4\)</span>, the new triplet is <span class="math notranslate nohighlight">\((x_4, x_2, x_3)\)</span></p></li>
<li><p>if <span class="math notranslate nohighlight">\(f(x_4) &lt; f(x_2)\)</span>, replace <span class="math notranslate nohighlight">\(x_3\)</span> with <span class="math notranslate nohighlight">\(x_2\)</span>, the new triplet is <span class="math notranslate nohighlight">\((x_1, x_4, x_2)\)</span></p></li>
<li><p>repeat until the width of the interval is smaller than your tolerance</p></li>
</ol>
</div>
<div class="figure align-default" id="fig-golden-ratio">
<img alt="_images/golden_search.png" src="_images/golden_search.png" />
<p class="caption"><span class="caption-number">Fig. 3 </span><span class="caption-text">Golden ratio method for a 1-variable function f(x) <a class="reference external" href="https://en.wikipedia.org/wiki/Golden-section_search#/media/File:GoldenSectionSearch.png">Figure source</a></span><a class="headerlink" href="#fig-golden-ratio" title="Permalink to this image">¶</a></p>
</div>
<p>The question is, what is the optimal way to pick the new point <span class="math notranslate nohighlight">\(x_4\)</span>? Since we have no information about the shape of the function <span class="math notranslate nohighlight">\(f(x)\)</span>, we don’t know where the minimum is more likely to be hiding. The best approach is to hedge your bets and make each option (3 or 4) result in the same interval scaled by the same factor.</p>
<p>Assume that we pick <span class="math notranslate nohighlight">\(x_4\)</span> in <span class="math notranslate nohighlight">\((x_1, x_2)\)</span>, as the larger interval, and  we want to pick <span class="math notranslate nohighlight">\(x_4\)</span> so that the length is reduced by the same factor in both cases: <span class="math notranslate nohighlight">\((x_4, x_2, x_3)\)</span> and <span class="math notranslate nohighlight">\((x_1, x_4, x_2)\)</span> and the resulting triplet is also divided in the same proportion. This is the condition for the golden ratio (that the ratio of the larger segment to the whole is the same as the ratio of the smaller segment to the larger). Thus, the  the choice for the new point is <span class="math notranslate nohighlight">\(x_4 = x_1 + (x_3 - x_1)/\phi\)</span>, where <span class="math notranslate nohighlight">\(\phi = (1 + √5)/2\)</span>. Similarly, if the interval <span class="math notranslate nohighlight">\((x_2,x_3)\)</span> is larger, we pick <span class="math notranslate nohighlight">\(x_4 = x_3 - (x_3 - x_1)/\phi\)</span>.</p>
<p>Notice that this means that the interval bracketing the minimum shrinks by a factor of <span class="math notranslate nohighlight">\(\phi \approx 1.61\)</span> every step.</p>
</div>
<div class="section" id="bisection-method">
<h3>bisection method<a class="headerlink" href="#bisection-method" title="Permalink to this headline">¶</a></h3>
<p>The bisection method is applicable for a continuous function <span class="math notranslate nohighlight">\(f(x)\)</span> which has a computable derivative function <span class="math notranslate nohighlight">\(F(x)\)</span>. In that case, finding a maximum or minimum of f(x) is the same as finding a root of the derivative function F(x). The bisection method simply divides the bracketing interval that contains the maximum or minimum into two, like this:</p>
<div class="figure align-default" id="fig-bisection">
<img alt="_images/bisection.png" src="_images/bisection.png" />
<p class="caption"><span class="caption-number">Fig. 4 </span><span class="caption-text">Bisection method involves cutting the interval bracketing a zero in two <a class="reference external" href="https://en.wikipedia.org/wiki/Bisection_method#/media/File:Bisection_method.svg">Figure source</a></span><a class="headerlink" href="#fig-bisection" title="Permalink to this image">¶</a></p>
</div>
<div class="tip admonition">
<p class="admonition-title">Bisection algorithm</p>
<ol class="simple">
<li><p>start with an interval <span class="math notranslate nohighlight">\((a,b)\)</span> with <span class="math notranslate nohighlight">\(F(a)\)</span> and <span class="math notranslate nohighlight">\(F(b)\)</span> of opposite signs</p></li>
<li><p>choose <span class="math notranslate nohighlight">\(c = (b-a)/2\)</span></p></li>
<li><p>pick the side on which the two endpoints are bracketing zero, so the new interval is either <span class="math notranslate nohighlight">\((a,c)\)</span> or <span class="math notranslate nohighlight">\((c,b)\)</span></p></li>
<li><p>repeat until the interval width is smaller than the tolerance</p></li>
</ol>
</div>
<p>In this case, the bracketing interval is always decreased by a factor of two, which is faster than the golden section search.</p>
</div>
<div class="section" id="secant-method">
<h3>secant method<a class="headerlink" href="#secant-method" title="Permalink to this headline">¶</a></h3>
<p>Secant method is also a root-finding method, and can thus be used to find optima of functions with a computable derivative. If the derivative function is f(x), then the algorithm is:</p>
<div class="tip admonition">
<p class="admonition-title">Secant algorithm</p>
<ol class="simple">
<li><p>start with a bracketing interval <span class="math notranslate nohighlight">\((x_0, x_1)\)</span></p></li>
<li><p>calculate the slope of the (secant) line that connects the two points <span class="math notranslate nohighlight">\(a = \frac{f(x_1) - f(x_0)}{x_1 - x_0}\)</span></p></li>
<li><p>pick the next value <span class="math notranslate nohighlight">\(x_i = x_{i-1} - af(x_{i-1})\)</span>, which is where the secant line crosses 0</p></li>
<li><p>repeat until <span class="math notranslate nohighlight">\(f(x_i)\)</span> is close enough to zero</p></li>
</ol>
</div>
<div class="figure align-default" id="fig-secant">
<img alt="_images/secant.png" src="_images/secant.png" />
<p class="caption"><span class="caption-number">Fig. 5 </span><span class="caption-text">The secant method uses the line connecting the two bracketing points to approximate the root <a class="reference external" href="https://en.wikipedia.org/wiki/Secant_method#/media/File:Secant_method.svg">Figure source</a></span><a class="headerlink" href="#fig-secant" title="Permalink to this image">¶</a></p>
</div>
<p>The secant method is faster to converge than bisection for most functions.</p>
</div>
<div class="section" id="newton-raphson-method">
<h3>Newton-Raphson method<a class="headerlink" href="#newton-raphson-method" title="Permalink to this headline">¶</a></h3>
<p>We will now consider an old method for finding roots of functions, devised by the Great Newton Himself, is also useful for finding maxima and minima of functions for which an exact formula is given. To use for optimization, it requires not only the knowledge of the derivative function <span class="math notranslate nohighlight">\(f(x)\)</span> (as for secand and bisection method) but also the knowledge of the second derivative function <span class="math notranslate nohighlight">\(f'(x)\)</span> of the function we want to optimize.</p>
<p>The idea is based on the first-order Taylor expansion of a function near a point <span class="math notranslate nohighlight">\(x_0\)</span>: <span class="math notranslate nohighlight">\(f(x_0 + \Delta x) = f(x_0) + \Delta x f'(x_0) + ...\)</span>. The dots indicate higher-order terms that we will ignore, for sufficiently small <span class="math notranslate nohighlight">\(\Delta x\)</span>. Now, suppose the function has a root (zero) at <span class="math notranslate nohighlight">\(x_0\)</span>, so <span class="math notranslate nohighlight">\(f(x_0) = 0\)</span>. Then, if we are at a point  near the root, <span class="math notranslate nohighlight">\(x_0 + \Delta x\)</span>, we can calculate how far away it is from the root, by simply solving for <span class="math notranslate nohighlight">\(\Delta x  = f(x_0 + \Delta x) /  f'(x_0)\)</span>. The idea of Newton’s method is to find the step size needed to reach the root, by using the value of the derivative at the current point <span class="math notranslate nohighlight">\(x_0 + \Delta x\)</span> (assuming the points are close enough that the slopes are almost equal). Then the steps for the algorithm are:</p>
<div class="tip admonition">
<p class="admonition-title">Newton-Raphson algorithm in 1 dimension</p>
<ol class="simple">
<li><p>Start at some point <span class="math notranslate nohighlight">\(x_0\)</span> (not the root, as above)</p></li>
<li><p>Let <span class="math notranslate nohighlight">\(x_{i+1} = x_i - f(x_i)/f'(x_i)\)</span></p></li>
<li><p>Repeat, until  <span class="math notranslate nohighlight">\(|x_{i+1} - x_i|&lt; \epsilon_{tol}\)</span>, where <span class="math notranslate nohighlight">\(\epsilon_{tol}\)</span> is the specified tolerance</p></li>
</ol>
</div>
<div class="figure align-default" id="fig-newton1d">
<img alt="_images/Newton_1D.png" src="_images/Newton_1D.png" />
<p class="caption"><span class="caption-number">Fig. 6 </span><span class="caption-text">Newton-Raphson method for finding roots uses the derivative of the function to find the intersection of the tangent line with the x-axis</span><a class="headerlink" href="#fig-newton1d" title="Permalink to this image">¶</a></p>
</div>
<p>The method turns out to be very efficient. Its only limitation is that it cannot find a root for which the derivative is zero (graphically, a root at which the graph of the function only touches the <span class="math notranslate nohighlight">\(x\)</span>-axis). Another complication is that, if the starting point is not close enough to a root, it can be hard to predict which root the method will find, but once it settles in near one, it will reach it quickly. If the value of the derivative <span class="math notranslate nohighlight">\(f'(x_i)\)</span> is small, then the method can bounce around, sometimes almost chaotically, so its efficiency strongly depends on starting in proximity to a root.</p>
<p>To be used for oiptimization of a function <span class="math notranslate nohighlight">\(F(x)\)</span>, we need to be able to calculate <span class="math notranslate nohighlight">\(F'(x) = f(x)\)</span> and <span class="math notranslate nohighlight">\(F''(x) = f'(x)\)</span> and the find the root(s) of <span class="math notranslate nohighlight">\(f(x)\)</span>. Then one has to check whether the extremum is a max or a min, which is straightforward.</p>
</div>
</div>
<div class="section" id="optimization-of-multivariable-functions-using-derivatives">
<h2>Optimization of multivariable functions using derivatives<a class="headerlink" href="#optimization-of-multivariable-functions-using-derivatives" title="Permalink to this headline">¶</a></h2>
<p>Let us now consider functions of multiple variables, which is the case for most real applications. Such, a function, e.g. <span class="math notranslate nohighlight">\(f(x,y)\)</span>, takes in values of the variables <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, and returns a single number. Graphically, this could be plotted as a surface, with the height at at any pair of coordinates <span class="math notranslate nohighlight">\((x,y)\)</span> given by <span class="math notranslate nohighlight">\(f(x,y)\)</span>. Intuitively, the goal of optimization is to find the lowest (or highest) point on the surface, at least in a neighborhood (see above discussion about difficulties of global optimization). Let us assume that we are searching for a minimum, as the story is completely equivalent for maximization.</p>
<div class="section" id="multivariable-optimization-problem">
<h3>multivariable optimization problem<a class="headerlink" href="#multivariable-optimization-problem" title="Permalink to this headline">¶</a></h3>
<p>A function of more than one variable is denoted by <span class="math notranslate nohighlight">\(f(\vec x)\)</span>, where <span class="math notranslate nohighlight">\(\vec x\)</span> is a vector of <span class="math notranslate nohighlight">\(n\)</span> variables <span class="math notranslate nohighlight">\(x_1, x_2, ..., x_n\)</span>. This means the function takes in a vector of several numbers and returns a scalar - a single number. One simple visual analogy is the function that gives elevation (height) for a given latitude and longitude <span class="math notranslate nohighlight">\((x,y)\)</span>. By plotting that function we would produce the shape of a mountain range or any other terrain, with the appropriate height at each point in the <span class="math notranslate nohighlight">\(x-y\)</span> plane.</p>
<p>The basic condition for finding the maximum or minimum of a function is known from basic calculus. For a function of <span class="math notranslate nohighlight">\(n\)</span> variables <span class="math notranslate nohighlight">\(f(x_1, x_2, ..., x_n)\)</span> to be at an optimum, all partial derivatives must vanish:</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial f(x_1, x_2, ..., x_n)}{\partial x_1} = \frac{\partial f(x_1, x_2, ..., x_n)}{\partial x_2} = ... = \frac{\partial f(x_1, x_2, ..., x_n)}{\partial x_n} = 0 \]</div>
<p>This gives a set of <span class="math notranslate nohighlight">\(n\)</span> equations to be solved for <span class="math notranslate nohighlight">\(n\)</span> unknowns. However, in practice optimization problems are rarely solved this way, because solving <span class="math notranslate nohighlight">\(n\)</span> nonlinear equations can be difficult, particularly for complicated functions that frequently arise in biology. However, one can use iterative methods to converge to the optimum using derivatives of the objective function.</p>
</div>
<div class="section" id="gradient-and-contours">
<h3>gradient and contours<a class="headerlink" href="#gradient-and-contours" title="Permalink to this headline">¶</a></h3>
<p>In order to describe the changes in multivariable functions, we need more than one number. Consider the slope on the surface of a mountain: it may be steep in one direction, and flat in another. In order to deal with this, <em>partial derivatives</em> are used. These represent the rate of change of <span class="math notranslate nohighlight">\(f\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> separately. Geometrically, this means the slope of the landscape we visualized above, if sliced in the <span class="math notranslate nohighlight">\(x\)</span> or <span class="math notranslate nohighlight">\(y\)</span> direction.</p>
<p><strong>Example:</strong> If <span class="math notranslate nohighlight">\(f(x,y) = x^2-2x+y^2+4x+5\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial f}{\partial x} = 2x -2 \\
\frac{\partial f}{\partial y} = 2y +4 
\end{split}\]</div>
<p>This allows us to define the multi-dimensional equivalent of the derivative: the <em>gradient</em> of a function,</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>The <em>gradient</em> of a function <span class="math notranslate nohighlight">\(f(\vec x)\)</span> of multiple variables <span class="math notranslate nohighlight">\(\vec x = x_1, .., x_n\)</span> is a vector with <span class="math notranslate nohighlight">\(n\)</span> components, each one the partial derivative with respect to the corresponding variable:</p>
<div class="math notranslate nohighlight">
\[ \nabla f (\vec x) = (\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n}) \]</div>
</div>
<p>To gain some geometric intuition about gradients, let us introduce the notion of contours of <span class="math notranslate nohighlight">\(f(\vec x)\)</span></p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>The <em>countours</em> or <em>level curves</em> of a function <span class="math notranslate nohighlight">\(f(\vec x)\)</span> of multiple variables <span class="math notranslate nohighlight">\(\vec x = x_1, .., x_n\)</span> are sets of values of <span class="math notranslate nohighlight">\(\vec x\)</span> that satisfy equations, for any given constant c:</p>
<div class="math notranslate nohighlight">
\[ f (\vec x) = c \]</div>
</div>
<p>These are curves (for two-variable functions) or surfaces (for three-variable functions) in the space of the variables on which the function is equal to a particular constant. One common example of this are contours of a piece of the Earth on a topographical map.</p>
<p>There is an important relationship between gradients and contours. Since there there is no change in <span class="math notranslate nohighlight">\(f\)</span> as one travels along it, the derivative of the function in the direction of the contour curve at any point is 0. As a consequence, the direction of the fastest change of <span class="math notranslate nohighlight">\(f\)</span> at any point,  <span class="math notranslate nohighlight">\(\nabla f\)</span>  is orthogonal to the level curve at that point.</p>
<p><strong>Example:</strong> Let us find the contours (level curves) for the function we gave above, which means the solution of the equation <span class="math notranslate nohighlight">\(f(x,y) = (x-1)^2+(y+2)^2 = c\)</span>. These are circles centered at <span class="math notranslate nohighlight">\((1,-2)\)</span>, with the radius depending on the value of <span class="math notranslate nohighlight">\(c\)</span>.</p>
<p>Let us compare the direction of the gradient at a point, let us say at <span class="math notranslate nohighlight">\((0,-2)\)</span>. This point is horizontally to the left of the center of the circular level curves. Thus, the tangent line to the circle at the point is vertical. The gradient is <span class="math notranslate nohighlight">\(\nabla f = (-2,0)\)</span>: a vertical vector, perpendicular to the direction of the circle at that point.</p>
</div>
<div class="section" id="gradient-descent">
<h3>gradient descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h3>
<p>The simplest idea for finding the lowest point in a valley is to follow the slope downward. In multiple dimensions, the direction of greatest downward change (steepest descent) is given by the gradient, which leads to the <em>gradient descent</em> algorithm.</p>
<p>The first method for minimization of a multidimensional function simply takes steps in the direction of the gradient, until the point is sufficiently close to the bottom of the valley. This is called the method of <em>gradient descent</em>:</p>
<div class="tip admonition">
<p class="admonition-title">Gradient descent algorithm</p>
<ol class="simple">
<li><p>Start at some point <span class="math notranslate nohighlight">\(\vec x_0\)</span>.</p></li>
<li><p>Compute the gradient at the current point <span class="math notranslate nohighlight">\(\vec x_i\)</span>, <span class="math notranslate nohighlight">\(\nabla f (\vec x_i)\)</span>. Find the minimum of the function <span class="math notranslate nohighlight">\(f\)</span> along that direction, which may be done through a one-dimensional search.</p></li>
<li><p>Take the next point to be <span class="math notranslate nohighlight">\(x_{i+1} = x_i + \alpha \nabla f (\vec x_i)\)</span>, where <span class="math notranslate nohighlight">\(\alpha\)</span> is the multiple found by the one-dimensional search.</p></li>
<li><p>Repeat until <span class="math notranslate nohighlight">\(|| \vec x_{i+1} - \vec x_i || &lt; \epsilon_{tol}\)</span>, where <span class="math notranslate nohighlight">\(\epsilon_{tol}\)</span> is the specified tolerance</p></li>
</ol>
</div>
<p>The gradient descent method, although intuitively simple, is in practice inefficient. Specifically, it has difficulties with descending into a long, narrow valley: it takes many short, zigzagging steps on the way to the lowest point. Relying on the steepest way down is not the best way of reaching the minimum.</p>
</div>
<div class="section" id="id1">
<h3>Newton-Raphson method<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>A different approach at finding the minimum of a multi-variable function is to extend the Newton-Raphson method from one variable to multiple. The idea is once again to convert the optimization problem for <span class="math notranslate nohighlight">\(f(\vec x)\)</span> into a root-finding problem for <span class="math notranslate nohighlight">\(\nabla f (\vec x)\)</span> where the gradient function plays the role of the derivative in the 1-dimensional Newton’s method. In order for this to work, we need an equivalent to the second derivative as well.</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>For a  function <span class="math notranslate nohighlight">\(f(\vec x)\)</span> of a vector variable <span class="math notranslate nohighlight">\(\vec x\)</span> with <span class="math notranslate nohighlight">\(n\)</span> components, the <em>Hessian</em> is a <span class="math notranslate nohighlight">\(n\)</span> by <span class="math notranslate nohighlight">\(n\)</span> matrix, with each element defined as the second partial with respect to two variables:</p>
<div class="math notranslate nohighlight">
\[ 
H_{i,j} = \frac{\partial^2 f}{\partial x_i \partial x_j} 
\]</div>
</div>
<p><strong>Example:</strong> For the same function we saw <span class="math notranslate nohighlight">\(f(x,y) = x^2-2x+y^2+4x+5\)</span>, with the gradient:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial f}{\partial x} = 2x -2 \\
\frac{\partial f}{\partial y} = 2y +4 
\end{split}\]</div>
<p>The Hessian requires computing two more partials for each of the elements of the gradient:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial^2 f}{\partial x^2} = 2 \\
\frac{\partial^2 f}{\partial x \partial y} = 0 \\
\frac{\partial^2 f}{\partial y \partial x} = 0 \\
\frac{\partial^2 f}{\partial y^2} = 2
\end{split}\]</div>
<p>So the Hessian matrix is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H(f(\vec x)) = \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}
\end{split}\]</div>
<p>Now that we have the multi-variable equivalents of <span class="math notranslate nohighlight">\(f'(x)\)</span> and <span class="math notranslate nohighlight">\(f''(x)\)</span>, we can formulate the multidimensional Newton-Raphson algorithm using vectors and matrices:</p>
<div class="tip admonition">
<p class="admonition-title">Multi-variable Newton-Raphson algorithm</p>
<ol class="simple">
<li><p>Define the gradient  function <span class="math notranslate nohighlight">\(\nabla f (\vec x)\)</span> and  the Hessian function <span class="math notranslate nohighlight">\(H(\vec x)\)</span></p></li>
<li><p>Start at some point <span class="math notranslate nohighlight">\(\vec x_0\)</span> (as long as it is not the minimum)</p></li>
<li><p>Update the point using<span class="math notranslate nohighlight">\(\vec x_{i+1} = \vec x_i - H^{-1}(\vec x_i) \nabla f(\vec x_i)\)</span></p></li>
<li><p>Repeat until <span class="math notranslate nohighlight">\(|| \vec x_{i+1} - \vec x_i || &lt; \epsilon_{tol}\)</span>, where <span class="math notranslate nohighlight">\(\epsilon_{tol}\)</span> is the specified tolerance</p></li>
</ol>
</div>
<p>The geometric intuition of the Newton-Raphson method relies on the fact that the second derivative (Hessian) matrix <span class="math notranslate nohighlight">\(H\)</span> represents the curvature of the objective function, similar to the coefficient <span class="math notranslate nohighlight">\(a\)</span> of the 1-variable quadratic function <span class="math notranslate nohighlight">\(ax^2 + bx + c\)</span>. Instead of simply using the gradient, or the linear component of the objective function, this method uses the curvature to speed up the search. In fact, Newton-Raphson performs very efficiently once it is within a nearly quadratic-shaped well, but doesn’t do very well far from the minimum.</p>
</div>
<div class="section" id="levenberg-marquardt-method">
<h3>Levenberg-Marquardt method<a class="headerlink" href="#levenberg-marquardt-method" title="Permalink to this headline">¶</a></h3>
<p>The Levenberg-Marquardt method is a combination of the gradient descent and Newton-Raphson methods. It uses their respective strengths, by using the gradient to quickly reach the vicinity (approximately quadratic valley) of a minimum, and then use the curvature information from the Hessian to finish the job efficiently. The insight of this method is that one can interpolate between the two methods by introducing a parameter <span class="math notranslate nohighlight">\(\lambda\)</span> which at high values weigh the method toward gradient descent, and at small values the Hessian and therefore Newton’s method dominates. Here is an outline of the algorithm, which starts with a large value of <span class="math notranslate nohighlight">\(\lambda\)</span>, assuming that the initial guess <span class="math notranslate nohighlight">\(\vec x_0\)</span> is far from the minimum:</p>
<div class="tip admonition">
<p class="admonition-title">Levenberg-Marquardt algorithm</p>
<ol class="simple">
<li><p>Define the gradient  function <span class="math notranslate nohighlight">\(\nabla f (\vec x)\)</span> and  the Hessian function <span class="math notranslate nohighlight">\(H(\vec x)\)</span></p></li>
<li><p>Start at some point <span class="math notranslate nohighlight">\(\vec x_0\)</span> (as long as it is not the minimum) and set <span class="math notranslate nohighlight">\(\lambda\)</span> to be large (e.g. 1000)</p></li>
<li><p>Update the point using <span class="math notranslate nohighlight">\(\vec x_{i+1} = \vec x_i - \left(H(\vec x_i) + \lambda diag[H(\vec x_i) ] \right)^{-1} \nabla f(\vec x_i)\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(f(\vec x_{i+1}) &lt; f(\vec x_i) \)</span> (the new point is an improvement), accept <span class="math notranslate nohighlight">\(\vec x_{i+1}\)</span> and divide <span class="math notranslate nohighlight">\(\lambda\)</span> by a factor <span class="math notranslate nohighlight">\(S\)</span> (which can be set to be any positive real number, usually in the range of 2-10)</p></li>
<li><p>else (if the point is not an improvement) keep <span class="math notranslate nohighlight">\(\vec x_i\)</span> as the current point  and multiply <span class="math notranslate nohighlight">\(\lambda\)</span> by <span class="math notranslate nohighlight">\(S\)</span></p></li>
<li><p>Repeat until <span class="math notranslate nohighlight">\(|| \vec x_{i+1} - \vec x_i || &lt; \epsilon_{tol}\)</span>, where <span class="math notranslate nohighlight">\(\epsilon_{tol}\)</span> is the specified tolerance</p></li>
</ol>
</div>
<p>When <span class="math notranslate nohighlight">\(\lambda=0\)</span>, the updating expression is identical to the Newton-Raphson step. One unusual insight in the method is the use of the diagonal of the Hessian instead of just the identity matrix, which is in line with the gradient descent updating step. It turns out that using the diagonal of the Hessian matrix (which contains the second derivatives w.r.t. to all the variables) guarantees more efficient convergence than using just the identity matrix by using the geometry of the curvature to scale the gradient accordingly. This method is widely used in nonlinear least squares fitting problems, such as fitting data to exponential or other complex functions.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="linear_regression.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Linear regression</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="PCA.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Principal Component Analysis</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Dmitry Kondrashov<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>