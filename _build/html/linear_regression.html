
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Linear regression &#8212; Mathematical Methods for Biological Sciences (part 2)</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Optimization using gradients" href="gradient_optimization.html" />
    <link rel="prev" title="Mathematical Methods for Biology Part 2: Algorithms" href="intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Mathematical Methods for Biological Sciences (part 2)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Mathematical Methods for Biology Part 2: Algorithms
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gradient_optimization.html">
   Optimization using gradients
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PCA.html">
   Principal Component Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="fourier_series.html">
   Fourier series: decomposition by frequency
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/linear_regression.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Flinear_regression.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#list-of-terms-and-concepts">
   List of terms and concepts
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#systems-of-linear-equations">
   Systems of linear equations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#invertibility-of-matrices">
     invertibility of matrices
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fitting-a-line-to-data">
   Fitting a line to data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#minimizing-the-sum-of-residuals">
     minimizing the sum of residuals
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assumptions-of-linear-regression">
   assumptions of linear regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-least-squares-for-polynomial-fitting">
   linear least squares for polynomial fitting
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Linear regression</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#list-of-terms-and-concepts">
   List of terms and concepts
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#systems-of-linear-equations">
   Systems of linear equations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#invertibility-of-matrices">
     invertibility of matrices
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fitting-a-line-to-data">
   Fitting a line to data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#minimizing-the-sum-of-residuals">
     minimizing the sum of residuals
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assumptions-of-linear-regression">
   assumptions of linear regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-least-squares-for-polynomial-fitting">
   linear least squares for polynomial fitting
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="linear-regression">
<h1>Linear regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h1>
<p>One of the most common ways of either fitting data, or if you want to put it in a fancier way, train a machine learning model, is called linear regression. This starts with a data set that has two different variables and pairs of observations of each, and produce a linear model that uses one variable (called explanatory) to predict the other (called response). Graphically speaking, the goal is to plot a line on a scatterplot that best fits the data (in one variable).</p>
<p>Though it is generally not possible to produce an exact fit for more than two observations, there is a method to calculate the closest linear model, called least-squares fitting. We will develop some fundamental tools from linear algebra to do this calculation, and then talk about the underlying assumptions and what they mean for applicability of linear regression.</p>
<div class="section" id="list-of-terms-and-concepts">
<h2>List of terms and concepts<a class="headerlink" href="#list-of-terms-and-concepts" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Solving linear equations</p></li>
<li><p>Matrix inverse</p></li>
<li><p>Least-squares data fitting</p></li>
<li><p>Explanatory vs. response variables and supervised learning</p></li>
<li><p>Covariance and correlation</p></li>
<li><p>Goodness of fit and R-squared</p></li>
<li><p>Polynomial regression</p></li>
<li><p>Residuals and assumptions of linear regression</p></li>
</ul>
</div>
<div class="section" id="systems-of-linear-equations">
<h2>Systems of linear equations<a class="headerlink" href="#systems-of-linear-equations" title="Permalink to this headline">¶</a></h2>
<p>As one goes through life, sometimes one has to solve a set of linear equations, that have multiple variables (let’s call them <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>) and the same number of equations that they need to satisfy with constant coefficients. For example, here is a system of two linear equations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
2a - b = -3 \\
a + b  = 1
\end{split}\]</div>
<p>where we want to find <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> that satisfy both equations. This can be written as a matrix equation, with matrix <span class="math notranslate nohighlight">\(M\)</span> containing the coefficients on the left hand side and the vector <span class="math notranslate nohighlight">\(\vec v\)</span> containing the two coefficients on the right hand side, and the vector <span class="math notranslate nohighlight">\(\vec a\)</span> containing the unknown variables <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
\begin{pmatrix} 2 &amp; -1 \\ 1 &amp; 1 \end{pmatrix} \times \begin{pmatrix} a \\ b\end{pmatrix} =&amp; \begin{pmatrix}-3 \\1\end{pmatrix} \\
M \vec a =&amp;  \vec v 
\end{split}\]</div>
<p>Written as a single linear equation, it is tempting to “divide” both sides by <span class="math notranslate nohighlight">\(M\)</span> and thus solve for the vector <span class="math notranslate nohighlight">\(\vec a\)</span>, but matrices cannot be reciprocated like numbers. Linear algebra provides a way of doing this correctly.</p>
<p>In order to get rid of the matrix <span class="math notranslate nohighlight">\(M\)</span> on one side of the equation, one can multiply it by another matrix called its inverse.</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>For a square (<span class="math notranslate nohighlight">\(n\)</span> by <span class="math notranslate nohighlight">\(n\)</span>) matrix <span class="math notranslate nohighlight">\(M\)</span> the <em>inverse</em> matrix <span class="math notranslate nohighlight">\(M^{-1}\)</span> (also <span class="math notranslate nohighlight">\(n\)</span> by <span class="math notranslate nohighlight">\(n\)</span>) satisfies the following conditions: <span class="math notranslate nohighlight">\(M^{-1} \times M = M \times M^{-1} = I\)</span>, where <span class="math notranslate nohighlight">\(I\)</span> is the <span class="math notranslate nohighlight">\(n\)</span> by <span class="math notranslate nohighlight">\(n\)</span> identity matrix.</p>
</div>
<p>Example: For the matrix above, the inverse matrix is (check for yourself)</p>
<div class="math notranslate nohighlight">
\[\begin{split} M^{-1} = \begin{pmatrix}1/3 &amp; 1/3 \\ -1/3 &amp; 2/3\end{pmatrix} \end{split}\]</div>
<p>In general, finding the inverse of a matrix is best left to computers. However, for a 2 by 2 matrix, there is an explicit formula for an inverse:</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
M = \begin{pmatrix} \alpha &amp; \beta \\ \gamma &amp; \delta \end{pmatrix}  \\
M^{-1} = \frac{1}{\det(M)} \begin{pmatrix} \delta &amp; -\beta \\ -\gamma &amp; \alpha \end{pmatrix}
\end{split}\]</div>
<p>where the determinant <span class="math notranslate nohighlight">\(\det(M) = \alpha \delta - \beta \gamma\)</span>. Note that the division by the determinant of <span class="math notranslate nohighlight">\(M\)</span> in front of the matrix means every element of <span class="math notranslate nohighlight">\(M\)</span> is divided by determinant (as we see in the example above, where every element is divided by 3).</p>
<p>Once we have found the inverse of a matrix, we can solve the linear equation by multiplying both sides by the inverse:</p>
<div class="math notranslate nohighlight">
\[\begin{split} M^{-1} \times M \times \vec a = V^{-1} \times \vec v \\ 
\vec a =  M^{-1} \times \vec v  \end{split}\]</div>
<p>In the example above, we multiply the vector <span class="math notranslate nohighlight">\(\vec v\)</span> by the inverse and find the solution: <span class="math notranslate nohighlight">\((a,b) = (-2/3, 5/3)\)</span> (you can check that it works by plugging it into the original equations)</p>
<div class="section" id="invertibility-of-matrices">
<h3>invertibility of matrices<a class="headerlink" href="#invertibility-of-matrices" title="Permalink to this headline">¶</a></h3>
<p>It is useful to consider the geometric meaning of systems of linear equations. In two dimensions, as in the above example, each equation can be represented by a line in the plane. The solution to the two equations is the intersection of the two lines. The intersection is guaranteed to exist if the two lines are not parallel. If they are indeed parallel, then they either do not intersect at all, so there is no solution, or they overlap completely, in which case there are infinitely many solutions.</p>
<p>A similar geometric interpretation is true in higher dimensions. In three dimensions, each linear equation represents a plane, and as long as no two planes are parallel, there is only one point in which they intersect. But if two planes have the same direction, again, there is either no solution, or infinitely many (a line or plane of solutions). In higher dimensions, a solution is the intersection of <span class="math notranslate nohighlight">\(n\)</span> hyper-planes, and again, for a unique solutions to exist, no two hyper-planes can be parallel.</p>
<p>We saw the algebraic and geometric approach to solving systems of linear equations. In the algebraic solution, we can multiply by the inverse of the matrix, but we did not specify when it exists. Algebraically speaking, this can be determined from the determinant of the matrix, as in the formula for the inverse of a 2 by 2 matrix. This is the reason for the following fundamental result:</p>
<div class="admonition-theorem admonition">
<p class="admonition-title">Theorem</p>
<p>Invertibility property: For a square (<span class="math notranslate nohighlight">\(n\)</span> by <span class="math notranslate nohighlight">\(n\)</span>) matrix <span class="math notranslate nohighlight">\(M\)</span>, an inverse matrix <span class="math notranslate nohighlight">\(M^{-1}\)</span> (also <span class="math notranslate nohighlight">\(n\)</span> by <span class="math notranslate nohighlight">\(n\)</span>) exists if any only if the determinant of <span class="math notranslate nohighlight">\(M\)</span> is not zero.</p>
</div>
<p>Geometrically speaking, a determinant of zero indicates that the intersection of the lines (or hyperplanes) is not a single point or speaking mathematically, they are not linearly independent. If that is the case, as we said above, there is not unique solution to the system of equations: there are either none, or infinitely many solutions.</p>
</div>
</div>
<div class="section" id="fitting-a-line-to-data">
<h2>Fitting a line to data<a class="headerlink" href="#fitting-a-line-to-data" title="Permalink to this headline">¶</a></h2>
<p>One of the most common questions in data science (or any science) is to describe a relationship between two numeric variables. Often, one is seen as the potential cause and the other as the effect, and they are called the explanatory and response variables, respectively. For example, <a class="reference internal" href="#fig-cancer-risk"><span class="std std-numref">Fig. 1</span></a> plots multiple data points of the cancer risk for different types of tissues plotted on the y-axis (response) as a function of the total number of cell divisions plotted on the x-axis (explanatory).</p>
<div class="figure align-default" id="fig-cancer-risk">
<img alt="_images/cancer_lin_reg.jpeg" src="_images/cancer_lin_reg.jpeg" />
<p class="caption"><span class="caption-number">Fig. 1 </span><span class="caption-text">Cancer risk (response) as a function of number of cell divisions (explanatory); <a class="reference external" href="https://www.science.org/doi/10.1126/science.1260825">Figure from</a></span><a class="headerlink" href="#fig-cancer-risk" title="Permalink to this image">¶</a></p>
</div>
<p>The question is: can the relationship between the variables be described by a linear function <span class="math notranslate nohighlight">\(y = ax + b\)</span>? And if so, how do you choose the best slope <span class="math notranslate nohighlight">\(a\)</span> and intercept <span class="math notranslate nohighlight">\(b\)</span>?</p>
<p>The answer is straightforward if we only have two data points: we can use the exact solution that we described in the previous section. For example, if the two data points are  <span class="math notranslate nohighlight">\((-1, -2), (5, 4)\)</span>, then the line that passes through both points must satisfy both equations below, with <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> being the slope and the intercept:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
-a + b = -2 \\
5a + b  = 4
\end{split}\]</div>
<p>To find the solution for <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, we take the inverse of the matrix of coefficients <span class="math notranslate nohighlight">\(M\)</span> and multiply it by the vector <span class="math notranslate nohighlight">\(\vec v\)</span> on the left hand side:</p>
<div class="math notranslate nohighlight">
\[\begin{split} M = \begin{pmatrix} -1 &amp; 1 \\ 5 &amp; 1\end{pmatrix}; \vec v =  \begin{pmatrix} -2 \\ 4 \end{pmatrix}\\
M^{-1} \times \vec v  = \frac{1}{-6} \begin{pmatrix} 1 &amp; -1 \\ -5 &amp; -1 \end{pmatrix} \times  \begin{pmatrix} -2 \\ 4 \end{pmatrix} =  \begin{pmatrix} 1\\ -1 \end{pmatrix} \end{split}\]</div>
<p>This means that a line with slope 1 and intercept -1 will pass through these two points.</p>
<p>But of course two data points is a very small amount of data to build a model. To make it just a bit more interesting, let’s add one more data point, so our data set is: <span class="math notranslate nohighlight">\((-1, -2), (5, 4), (2,7)\)</span>. How can we find a line to fit those points?</p>
<p>Bad idea: Take two points and find a line, that is the slope and the intercept, that passes through the two. It should be clear why this is a bad idea: we are arbitrarily ignoring some of the data, while perfectly fitting two points.</p>
<p>So how do we use all the data? Let us write down the equations that a line with slope <span class="math notranslate nohighlight">\(a\)</span> and intercept <span class="math notranslate nohighlight">\(b\)</span> have to satisfy in order to fit our data points:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
-a + b = -2 \\
5a + b = 4 \\
2a + b = 7
\end{split}\]</div>
<p>Let us write it in matrix form again:</p>
<div class="math notranslate nohighlight">
\[\begin{split}  
\begin{pmatrix} -1 &amp; 1 \\ 5 &amp; 1 \\ 2 &amp; 1\end{pmatrix} \times \begin{pmatrix} a \\b\end{pmatrix} = \begin{pmatrix} -2 \\ 4 \\ 7 \end{pmatrix} \\
M \times \begin{pmatrix} a \\ b\end{pmatrix} =  \vec v
\end{split}\]</div>
<p>This system has no exact solution, since there are three equations and only two unknowns. We need to find <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> such that they provide the <em>best fit</em> to the data, not the perfect solution. To do that, we need to define how to measure goodness of fit.</p>
<div class="section" id="minimizing-the-sum-of-residuals">
<h3>minimizing the sum of residuals<a class="headerlink" href="#minimizing-the-sum-of-residuals" title="Permalink to this headline">¶</a></h3>
<p>The most common approach to determine the goodness of fit is to subtract the predicted values of <span class="math notranslate nohighlight">\(y\)</span> from the data, as follows: <span class="math notranslate nohighlight">\(e_i = y_i - (mx_i + b)\)</span>. However, if we add it all up, the errors with opposite signs will cancel each other, giving the impression of a good fit simply if the deviations are symmetric. A more reasonable approach is to take absolute values of the deviations before adding them up. This is called the total deviation, for <span class="math notranslate nohighlight">\(n\)</span> data points with a line fit:</p>
<div class="math notranslate nohighlight">
\[ TD = \sum_{i=1}^n |  y_i - mx_i - b | \]</div>
<p>Mathematically, a better measure of total error is a sum of squared errors, which also has the advantage of adding up nonnegative values, but is known as a better measure of the distance between the fit and the data (think of Euclidean distance, which is also a sum of squares):</p>
<div class="math notranslate nohighlight">
\[ SSE = \sum_{i=1}^n ( y_i - mx_i - b )^2 \]</div>
<p>To calculate the best-fit slope and intercept, we first need to define the variance and covariance of a data set:</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>The <em>variance</em> of a data set <span class="math notranslate nohighlight">\(X\)</span> with <span class="math notranslate nohighlight">\(n\)</span> data points is the following sum, where <span class="math notranslate nohighlight">\(\bar X\)</span> is the mean of the data:</p>
<div class="math notranslate nohighlight">
\[
Var(X) = \frac{1}{n-1} \sum_{i=1}^n (\bar X - x_i)^2
\]</div>
<p>The covariance of a data set of pairs of values <span class="math notranslate nohighlight">\((X,Y)\)</span> is the sum of the products of the corresponding deviations from their respective means:</p>
<div class="math notranslate nohighlight">
\[ Cov(X,Y) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar X) (y_i -\bar Y) \]</div>
</div>
<p>Intuitively, this means that if two variable tend to deviate in the same direction from their respective means, they have a positive covariance, and if they tend to deviate in opposite directions from their means, they have a negative covariance. In the intermediate case, if sometimes they deviate together and other times they deviate in opposition, the covariance is small or zero. For instance, the covariance between two independent random variables is zero.</p>
<p>It should come as no surprise that the slope of the linear regression depends on the covariance, that is, the degree to which the two variables deviate together from their means. If the covariance is positive, then for larger values of <span class="math notranslate nohighlight">\(x\)</span>  the corresponding <span class="math notranslate nohighlight">\(y\)</span> values tend to be larger, which means the slope of the line is positive. Conversely, if the covariance is negative, so is the slope of the line. And if the two variables are independent, the slope has to be close to zero. The actual formula for the slope of the linear regression is:</p>
<div class="math notranslate nohighlight">
\[
a = \frac{Cov(X,Y)}{Var(X)}
\]</div>
<p>To find the intercept of the linear regression, we make use of one other property of the best fit line: in order for it to minimize the SSE, it must pass through the point <span class="math notranslate nohighlight">\((\bar X, \bar Y)\)</span>. Again, I will not prove this, but note that the point of the two mean values is the central point of the “cloud” of points in the scatterplot, and if the line missed that central point, the deviations will be larger. Assuming that is the case, we have the following equation for the line: <span class="math notranslate nohighlight">\(\bar Y = a\bar X + b\)</span>, which we can solve for <span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="math notranslate nohighlight">
\[
b = \bar Y - \frac{Cov(X,Y) \bar X}{Var(X)}
\]</div>
<p>The parameters of the best-fit line can be calculated from the means, variances, and covariance of the two variable data set. But where did the formulas come from?</p>
<p>We want find the slope and intercept (<span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(b\)</span>) which result in the lowest sum of squared errors. This approach is generally known as least squares fitting, and in the case of fitting a line, it is called linear <em>regression</em>. One way to find the values that minimize the sum of squared errors is to find the derivatives of SSE with respect to <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(b\)</span> and set them to 0:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac {\partial SSE}{\partial m}  = \sum_{i=1}^n -2x_i( y_i - ax_i - b ) = 0 \\
\frac {\partial SSE}{\partial b}  = \sum_{i=1}^n -2( y_i - ax_i - b ) = 0
\end{split}\]</div>
<p>Re-write this with the <span class="math notranslate nohighlight">\(y_i\)</span>s on the right hand side:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
a \sum_{i=1}^n  x_i^2 +  b \sum_{i=1}^n x_i  = \sum_{i=1}^n x_i y_i  \\
a \sum_{i=1}^n  x_i +  b \sum_{i=1}^n 1  = \sum_{i=1}^n  y_i 
\end{split}\]</div>
<p>This is now a linear system of equations, just as we started with. Turns out, there is compact way of representing this equation in matrix notation. Using the notation from the example above, let
the matrix <span class="math notranslate nohighlight">\(M\)</span> contain a column of <span class="math notranslate nohighlight">\(x\)</span> values from the data, and a column of ones, and the vector <span class="math notranslate nohighlight">\(\vec y\)</span> contain a column of <span class="math notranslate nohighlight">\(y\)</span> values of the data:</p>
<div class="math notranslate nohighlight">
\[\begin{split} M = \begin{pmatrix} x_1 &amp; 1 \\... &amp; ... \\x_n &amp; 1\end{pmatrix}; \; \vec y = \begin{pmatrix} y_1 \\... \\y_n \end{pmatrix} \end{split}\]</div>
<p>Then the equations above can be written as the following linear algebra equation, and solved using matrix inverse:</p>
<div class="math notranslate nohighlight">
\[\begin{split} M^t \times M \times \begin{pmatrix} a \\ b\end{pmatrix}  = M^t \times \vec y \\
\begin{pmatrix} a \\ b \end{pmatrix}  = (M^t \times M)^{-1} \times M^t \times  \vec y \end{split}\]</div>
<p>There is a linear algebra fact that the 2 by 2 matrix <span class="math notranslate nohighlight">\(M^t \times M\)</span> is invertible so long as the columns of <span class="math notranslate nohighlight">\(M\)</span> are linearly independent. In this case this means as long as the <span class="math notranslate nohighlight">\(x\)</span> values of the data are not all the same, we can find a least-squares linear fit to a set of <span class="math notranslate nohighlight">\(n\)</span> data points. If you write down the solution for <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> as sums of all the components, you will obtain the formulas that were presented above.</p>
<p>One essential measure of the quality of linear regression is correlation, which is a measure of how much variation in one random variable corresponds to variation in the other. If this sounds very similar to the description of covariance, it’s because they are closely related. Essentially, correlation is normalized covariance, made to range between -1 and 1. Here is the definition:</p>
<div class="admonition-definition admonition">
<p class="admonition-title">Definition</p>
<p>The (linear or Pearson) correlation of a data set of pairs of data values <span class="math notranslate nohighlight">\((X,Y)\)</span> is:</p>
<div class="math notranslate nohighlight">
\[ r = \frac{Cov(X,Y)}{\sqrt{{Var(X)}{Var(Y)}}} =  \frac{Cov(X,Y)}{\sigma_X \sigma_Y}\]</div>
</div>
<p>If the two variables are identical, <span class="math notranslate nohighlight">\(X=Y\)</span>, then the covariance becomes its variance <span class="math notranslate nohighlight">\(Cov(X,Y) = Var(X)\)</span> and the denominator also becomes the variance, and the correlation is 1. This is also true if <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are scalar multiples of each other, as you can see by plugging in <span class="math notranslate nohighlight">\(X= cY\)</span> into the covariance formula. The opposite case if <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are diametrically opposite, <span class="math notranslate nohighlight">\(X = -cY\)</span>, which has the correlation coefficient of -1. All other cases fall in the middle, neither perfect correlation nor perfect anti-correlation. The special case if the two variables are independent, and thus their covariance is zero, has the correlation coefficient of 0.</p>
<p>This gives a connection between correlation and slope of linear regression:</p>
<div class="math notranslate nohighlight">
\[
a = r \frac{\sigma_Y}{\sigma_X}
\]</div>
<p>Whenever linear regression is reported, one always sees the values of correlation <span class="math notranslate nohighlight">\(r\)</span> and squared correlation <span class="math notranslate nohighlight">\(r^2\)</span> displayed. The reason for this is that <span class="math notranslate nohighlight">\(r^2\)</span> has the meaning of the the fraction of the variance of the dependent variable <span class="math notranslate nohighlight">\(Y\)</span> explained by the linear regression <span class="math notranslate nohighlight">\(Y=aX+b\)</span>.</p>
<div class="figure align-default" id="fig-corr-examples">
<img alt="_images/Correlation_examples.png" src="_images/Correlation_examples.png" />
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text">Correlation coefficient does not tell the whole story when it comes to describing the relationship between two variables; multiple scatterplots of generated data with correlation coefficient r shown above.<a class="reference external" href="http://en.wikipedia.org/wiki/File:Correlation_examples2.svg">from Wikimedia</a></span><a class="headerlink" href="#fig-corr-examples" title="Permalink to this image">¶</a></p>
</div>
<p>There are, as usual, a couple of cautions about relying on the correlation coefficient First, just because there is no linear relationship, does not mean that there is no other relationship.  <a class="reference internal" href="#fig-corr-examples"><span class="std std-numref">Fig. 2</span></a> shows some examples of scatterplots and their corresponding correlation coefficients. What it shows is that while a formless blob of a scatterplot will certainly have zero correlation, so will other scatterplots in which there is a definite relationship (e.g. a circle, or a X-shape). The point is that <strong>correlation is always a measure of the linear relationship between variables</strong>.</p>
<p>Second cautionary tale is well known, as that is the danger of  equating correlation with a causal relationship. There are numerous examples of scientists misinterpreting a coincidental correlation as meaningful, or deeming two variables that have a common source as causing one another. It cannot be repeated often enough that one must be careful when interpreting correlation: a weak one does not mean there is no relationship, and a strong one does not mean that one variable causes the variation in the other.</p>
</div>
</div>
<div class="section" id="assumptions-of-linear-regression">
<h2>assumptions of linear regression<a class="headerlink" href="#assumptions-of-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>The simple formulas for slope, intercept, and standard deviation are only valid under certain conditions. The classic linear regression presented above relies on the following assumptions:</p>
<ul class="simple">
<li><p>the two variables have a linear relationship</p></li>
<li><p>the measurements are all independent of each other</p></li>
<li><p>there is no noise in the measurements of the independent variable</p></li>
<li><p>the noise in the measurements of the dependent variable is normally distributed with the same variance</p></li>
</ul>
<p>In reality, each data measurement has a random component, that we can call noise, resulting from experimental error, environmental variation, etc, and different measurements may have different levels of noise (standard deviation). One can estimate the error for a measurement, for instance by repeating the experiment several times, and estimating the standard deviation of the measurement random variable (we will not get into how to do this until the third quarter). It is important to account for this uncertainty in the data, since a measurement which is all over the place must carry less weight than one which is solid. A proper mathematical way of doing this is by defining a different function to measure the goodness of fit, known as the chi-squared function:</p>
<div class="math notranslate nohighlight">
\[ 
\chi^2 = \sum_{i=1}^n \frac{( y_i - ax_i - b )^2 }{\sigma_i^2} 
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma_i\)</span> is the standard deviation of the <span class="math notranslate nohighlight">\(i\)</span>-th data point. Given all this information, we can find a solution analogous to the one found in the previous section. The only modification is to divide the matrices by the standard deviation <span class="math notranslate nohighlight">\(\sigma_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
M = \begin{pmatrix} x_1/\sigma_i &amp; 1/\sigma_1 \\... &amp; ... \\x_n/\sigma_n &amp; 1/\sigma_n \end{pmatrix} \\
\vec y = \begin{pmatrix} y_1/\sigma_1 \\... \\y_n /\sigma_n \end{pmatrix} 
\end{split}\]</div>
<p>Then the least squares solution is found by the same formula as above, but here we have accounted for the experimental uncertainty:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{pmatrix} a \\b\end{pmatrix}  = (M^t \times M)^{-1} \times M^t \times \vec y \end{split}\]</div>
</div>
<div class="section" id="linear-least-squares-for-polynomial-fitting">
<h2>linear least squares for polynomial fitting<a class="headerlink" href="#linear-least-squares-for-polynomial-fitting" title="Permalink to this headline">¶</a></h2>
<p>Fitting data sets is not restricted to linear functions.  One simple extension is extension is to higher degree polynomials. Let us consider a quadratic function: <span class="math notranslate nohighlight">\(y = ax^2 + bx + c\)</span>. By analogy with the equations for fitting a linear function, we have a set of <span class="math notranslate nohighlight">\(n\)</span> equations, one for each data point:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
ax_1^2 + bx_1 + c  =  y_1 \\
ax_2^2 + bx_2 + c  =  y_2 \\
.... \\
ax_n^2 + bx_n + c  =  y_n 
\end{split}\]</div>
<p>Thus, we can define the matrix <span class="math notranslate nohighlight">\(M\)</span> for the least-squares quadratic fit, along with the same vector <span class="math notranslate nohighlight">\(\vec y\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split} M = \begin{pmatrix} x_1^2 &amp; x_1 &amp; 1 \\... &amp; ... &amp; ... \\x_n^2 &amp; x_n &amp; 1\end{pmatrix}; \; \vec y = \begin{pmatrix} y_1 \\... \\ y_n\end{pmatrix} \end{split}\]</div>
<p>and find the best fit parameters for the quadratic function:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{pmatrix}a \\b \\ c \end{pmatrix}  = (M^t \times M)^{-1} \times M^t \times \vec y \end{split}\]</div>
<p>It is straightforward to extend this to higher order polynomials, just by adding columns of higher powers of <span class="math notranslate nohighlight">\(x\)</span> data to the matrix <span class="math notranslate nohighlight">\(M\)</span>. The basic structure of the solution remains the same.</p>
<p>Another important concern is about the appropriate number of parameters in a fit for a particular data set. It is clear that adding more parameters results in better fit, but at some point the number of parameters is too large, and  “over-fitting” becomes as issue. Obviously, if one uses the same number of parameters as data points, one can obtain a perfect fit that has little predictive power - it just matches the given data. Deciding at what point adding more parameters is not productive is a difficult question, which can be addressed by various statistical methods that are outside of the scope of the course.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Mathematical Methods for Biology Part 2: Algorithms</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="gradient_optimization.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Optimization using gradients</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Dmitry Kondrashov<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>